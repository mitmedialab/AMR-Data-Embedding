{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Search for Embed Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from code_book_embed import *\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 1: Waveform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try every possible combination of pairs of waveforms on man and woman speech samples\n",
    "def waveform_optimize(waveform_list):\n",
    "    results = {}\n",
    "    \n",
    "    #paths_to_source = [\"audio_samples/man2_orig.wav\", \"audio_samples/woman2_orig.wav\"]\n",
    "    base_path = \"/audio_samples/Harvard_Sentences/\"\n",
    "    paths_to_source = [os.getcwd() + base_path + filename for filename in os.listdir(os.getcwd() + base_path)]\n",
    "    \n",
    "    results_ave_per_source = {}\n",
    "    for p in paths_to_source:\n",
    "        print \"Currently processing: \", p\n",
    "        for w1 in waveform_list:\n",
    "            for w2 in waveform_list:\n",
    "                if w1 == w2:\n",
    "                    continue\n",
    "                else:\n",
    "                    E2 = Embed(p, [w1, w2], [0,1], [0,1,0,1,0])\n",
    "\n",
    "                    # Fix the truncation and energy values\n",
    "                    E2.truncate(0.4, idx_list=[0,1])\n",
    "                    E2.energy(0.3, idx_list=[0])\n",
    "                    E2.energy(0.3, idx_list=[1])\n",
    "                    E2.pitch_shift(-15, idx_list=[1])\n",
    "                    E2.pitch_shift(-15, idx_list=[0])\n",
    "\n",
    "                    embed2, num_total_digits = E2.get_embedded_audio(plot=False)\n",
    "                    d_embed2, sr = compress_and_decompress(embed2, \"compression_samples/\", plot=False)\n",
    "\n",
    "                    # get the timeseries of the the original waveforms and recover\n",
    "                    wf = E2.get_data_timeseries()\n",
    "                    R2 = Recover(d_embed2, wf, [0,1], [0,1,0,1,0], num_total_digits)\n",
    "                    final_sequence2 = R2.get_bit_sequence(thres=0.85, plot=False)\n",
    "                    acc = R2.get_recovery_estimate(final_sequence2)\n",
    "                    metadata = str(p) + ':' + str(w1) + ':' + str(w2)\n",
    "                    results[metadata] = acc\n",
    "                    \n",
    "                    # results metrics average between speech samples\n",
    "                    try:\n",
    "                        metadata = str(w1) + ':' + str(w2)\n",
    "                        results_ave_per_source[metadata] += (float(acc) / len(paths_to_source))\n",
    "                    except KeyError:\n",
    "                        results_ave_per_source[metadata] = (float(acc) / len(paths_to_source))\n",
    "                    \n",
    "    return results, results_ave_per_source\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0011_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0019_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0016_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0012_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0060_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0031_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0061_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0017_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0059_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0030_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0010_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0040_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0057_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0034_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0032_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0035_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0014_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0039_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0018_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0015_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0038_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0036_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0037_8k.wav\n",
      "Currently processing:  /home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0013_8k.wav\n"
     ]
    }
   ],
   "source": [
    "waveform_list = [\"speech_samples/pronunciation_en_zero2.mp3\", \"speech_samples/pronunciation_en_one.mp3\", \n",
    "                 \"speech_samples/pronunciation_en_five.mp3\", \"speech_samples/pronunciation_en_seven.mp3\",\n",
    "                 \"speech_samples/pronunciation_en_nine.mp3\"]\n",
    "\n",
    "results_dict, results_ave_dict = waveform_optimize(waveform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( results_dict, open( \"results_waveform.p\", \"wb\" ))\n",
    "pickle.dump( results_ave_dict, open( \"results_ave_waveform.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per source\n",
      "('/home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0016_8k.wav:speech_samples/pronunciation_en_seven.mp3:speech_samples/pronunciation_en_nine.mp3', 0.83333333333333337) \n",
      "\n",
      "('/home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0057_8k.wav:speech_samples/pronunciation_en_one.mp3:speech_samples/pronunciation_en_seven.mp3', 0.90000000000000002) \n",
      "\n",
      "('/home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0014_8k.wav:speech_samples/pronunciation_en_seven.mp3:speech_samples/pronunciation_en_nine.mp3', 1.0) \n",
      "\n",
      "('/home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0038_8k.wav:speech_samples/pronunciation_en_five.mp3:speech_samples/pronunciation_en_seven.mp3', 1.0) \n",
      "\n",
      "('/home/ishwarya/Documents/math_modeling/AMR-Data-Embedding/audio_samples/Harvard_Sentences/OSR_us_000_0035_8k.wav:speech_samples/pronunciation_en_one.mp3:speech_samples/pronunciation_en_nine.mp3', 1.0) \n",
      "\n",
      "Average accuracy across sources\n",
      "('speech_samples/pronunciation_en_zero2.mp3:speech_samples/pronunciation_en_nine.mp3', 0.5796810259607622) \n",
      "\n",
      "('speech_samples/pronunciation_en_zero2.mp3:speech_samples/pronunciation_en_one.mp3', 0.5809396415988148) \n",
      "\n",
      "('speech_samples/pronunciation_en_zero2.mp3:speech_samples/pronunciation_en_seven.mp3', 0.58238942015183) \n",
      "\n",
      "('speech_samples/pronunciation_en_seven.mp3:speech_samples/pronunciation_en_nine.mp3', 0.5835416897916899) \n",
      "\n",
      "('speech_samples/pronunciation_en_five.mp3:speech_samples/pronunciation_en_seven.mp3', 0.591685572384102) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy per source\"\n",
    "sorted_res = sorted([(key, value) for key, value in results_dict.iteritems()], key = lambda x: x[1])\n",
    "for ele in sorted_res[-5:]:\n",
    "    print ele, \"\\n\"\n",
    "    \n",
    "print \"Average accuracy across sources\"\n",
    "sorted_res_ave = sorted([(key, value) for key, value in results_ave_dict.iteritems()], key = lambda x: x[1])\n",
    "for ele in sorted_res_ave[-5:]:\n",
    "    print ele, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 2: Length, Pitch, Energy - Simplex Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
